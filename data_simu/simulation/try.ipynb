{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _union(lists):\n",
    "    union_set = set()\n",
    "\n",
    "    for l in lists:\n",
    "        union_set = union_set.union(set(l))\n",
    "    return union_set\n",
    "\n",
    "def _is_conservative(elements, lists):\n",
    "    for e in elements:\n",
    "        conservative = False\n",
    "\n",
    "        for l in lists:\n",
    "            if e not in l:\n",
    "                conservative = True\n",
    "                break\n",
    "        if not conservative:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def _is_covering(elements, lists):\n",
    "    return set(elements) == _union(lists)\n",
    "\n",
    "def _pick_targets(nb_nodes, min_nb_target, max_nb_target, nb_interventions, nb_max_iteration=100000, cover=False, conservative=True):\n",
    "    nodes = np.arange(nb_nodes)\n",
    "    not_correct = True\n",
    "    i = 0\n",
    "\n",
    "    if(max_nb_target == 1):\n",
    "        intervention = np.sort(np.random.choice(nb_nodes, nb_interventions, replace=False))\n",
    "        targets = [[i] for i in intervention]\n",
    "\n",
    "    else:\n",
    "        while(not_correct and i < nb_max_iteration):\n",
    "            targets = []\n",
    "            not_correct = False\n",
    "            i += 1\n",
    "\n",
    "            # pick targets randomly (without repeat)\n",
    "            j = 0\n",
    "            while j < nb_interventions:\n",
    "                nb_targets = np.random.randint(min_nb_target, max_nb_target+1, 1)\n",
    "                intervention = np.sort(np.random.choice(nb_nodes, nb_targets, replace=False))\n",
    "                if intervention not in targets:\n",
    "                    targets.append(intervention)\n",
    "                    j += 1\n",
    "\n",
    "            # apply rejection sampling\n",
    "            if cover and not _is_covering(nodes, targets):\n",
    "                not_correct = True\n",
    "            if conservative and not _is_conservative(nodes, targets):\n",
    "                not_correct = True\n",
    "\n",
    "        if i == nb_max_iteration:\n",
    "            raise ValueError(\"Could generate appropriate targets. \\\n",
    "                                 Exceeded the maximal number of iterations\")\n",
    "\n",
    "        for i, t in enumerate(targets):\n",
    "            targets[i] = np.sort(t)\n",
    "\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from dynotears_data_generators.wrappers import gen_stationary_dyn_net_and_df, generate_dataframe_dynamic\n",
    "def gen_df_from_g(\n",
    "        g, p, intra_nodes, inter_nodes,\n",
    "        num_nodes, \n",
    "        n_samples = 100, \n",
    "        max_data_gen_trials = 10,\n",
    "        sem_type: str = \"linear-gauss\",\n",
    "        noise_scale: float = 1,\n",
    "        ): # generate time sery data\n",
    "    with np.errstate(over=\"raise\", invalid=\"raise\"):\n",
    "        burn_in = max(n_samples // 10, 50)\n",
    "\n",
    "        simulate_flag = True\n",
    "\n",
    "        while simulate_flag:\n",
    "            max_data_gen_trials -= 1\n",
    "            if max_data_gen_trials <= 0:\n",
    "                simulate_flag = False\n",
    "\n",
    "            try:\n",
    "                simulate_graphs_flag = False\n",
    "\n",
    "                # generate single time series\n",
    "                df = (\n",
    "                    generate_dataframe_dynamic(\n",
    "                        g,\n",
    "                        n_samples=n_samples + burn_in,\n",
    "                        sem_type=sem_type,\n",
    "                        noise_scale=noise_scale,\n",
    "                    )\n",
    "                    .loc[burn_in:, intra_nodes + inter_nodes]\n",
    "                    .reset_index(drop=True)\n",
    "                )\n",
    "\n",
    "                if df.isna().any(axis=None):\n",
    "                    continue\n",
    "            except (OverflowError, FloatingPointError):\n",
    "                continue\n",
    "            if (df.abs().max().max() < 1e3) or (max_data_gen_trials <= 0):\n",
    "                simulate_flag = False\n",
    "        if max_data_gen_trials <= 0:\n",
    "            warnings.warn(\n",
    "                \"Could not simulate data, returning constant dataframe\", UserWarning\n",
    "            )\n",
    "\n",
    "            df = pd.DataFrame(\n",
    "                np.ones((n_samples, num_nodes * (1 + p))),\n",
    "                columns=intra_nodes + inter_nodes,\n",
    "            )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from cdt.metrics import get_CPDAG\n",
    "\n",
    "def save_dag_cpdag(fname_radical, i_dataset, adjacency_matrix):\n",
    "    dag_path = os.path.join(fname_radical, f'DAG{i_dataset}.npy')\n",
    "    cpdag_path = os.path.join(fname_radical, f'CPDAG{i_dataset}.npy')\n",
    "    cpdag = get_CPDAG(adjacency_matrix)\n",
    "\n",
    "    np.save(dag_path, adjacency_matrix)\n",
    "    np.save(cpdag_path, cpdag)\n",
    "\n",
    "def save_data(fname_radical, i_dataset, data):\n",
    "        data_path = os.path.join(fname_radical, f'data{i_dataset}.npy')\n",
    "        np.save(data_path, data)\n",
    "\n",
    "def to_npy(fname_radical, i_dataset, adjacency_matrix, data):\n",
    "    \"\"\"\n",
    "    Save the generated data to the npy format,\n",
    "    in two separate files: data and the adjacency matrix of the\n",
    "    corresponding graph.\n",
    "\n",
    "    Args:\n",
    "        fname_radical (str): radical of the file names.\n",
    "        i_dataset (int): i-th dataset\n",
    "    \"\"\"\n",
    "    if not os.path.exists(fname_radical):\n",
    "        os.makedirs(fname_radical)\n",
    "    if data is not None:\n",
    "        save_dag_cpdag(fname_radical, i_dataset, adjacency_matrix)\n",
    "        save_data(fname_radical, i_dataset, data)\n",
    "    else:\n",
    "        raise ValueError(\"Graph has not yet been generated. \\\n",
    "                            Use self.generate() to do so.\")\n",
    "    \n",
    "def _save_data(folder, i, data, regimes=None, mask=None):\n",
    "    if mask is None:\n",
    "        data_path = os.path.join(folder, f'data{i+1}.npy')\n",
    "        np.save(data_path, data)\n",
    "    else:\n",
    "        data_path = os.path.join(folder, f'data_interv{i+1}.npy')\n",
    "        np.save(data_path, data)\n",
    "\n",
    "        data_path = os.path.join(folder, f'intervention{i+1}.csv')\n",
    "        with open(data_path, 'w', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(mask)\n",
    "\n",
    "    if regimes is not None:\n",
    "        regime_path = os.path.join(folder, f'regime{i+1}.csv')\n",
    "        with open(regime_path, 'w', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            for regime in regimes:\n",
    "                writer.writerow([regime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from causalnex.structure.structuremodel import StructureModel\n",
    "import pandas as pd\n",
    "from diff_intervene_generation.causal_mechanisms import UniformCause\n",
    "from dynotears_data_generators.wrappers import generate_dataframe_dynamic\n",
    "\n",
    "\n",
    "def simulate_timedata(\n",
    "    i_dataset = 1,\n",
    "    file_path = '/home/lpw/dcdi-master/data_timesery/imperfect/',\n",
    "    nb_interventions = 5,\n",
    "    min_nb_target=1, \n",
    "    max_nb_target=3,\n",
    "    nb_all_sample = 10,\n",
    "    num_nodes = 5,\n",
    "    nb_time_points = 100,\n",
    "    lag = 3,\n",
    "    interv_type = 'imperfect' # perfect\n",
    "    ):\n",
    "\n",
    "    # generate mask_intervention列表包含与每个点相关联的标记，regimes列表包含每个点所属的intervention编号, 共points_per_interv[j]*nb_time_points点\n",
    "    div = nb_interventions + 1\n",
    "    # one-liner taken from https://stackoverflow.com/questions/20348717/algo-for-dividing-a-number-into-almost-equal-whole-numbers/20348992\n",
    "    points_per_interv = [nb_all_sample // div + (1 if x < nb_all_sample % div else 0)  for x in range (div)]\n",
    "    # points_per_interv = int(self.nb_points / self.nb_interventions)\n",
    "    print('points_per_interv:', points_per_interv)\n",
    "    \n",
    "    # generate true data\n",
    "    g, df, intra_nodes, inter_nodes = gen_stationary_dyn_net_and_df(\n",
    "        num_nodes = num_nodes, n_samples = nb_time_points*points_per_interv[0],\n",
    "        p = lag, degree_intra = 3, degree_inter = 3,\n",
    "        graph_type_intra = \"erdos-renyi\", graph_type_inter = \"erdos-renyi\",\n",
    "        # w_min_intra = 0.5, w_max_intra = 2.0,\n",
    "        # w_min_inter = 0.3, w_max_inter = 0.5,\n",
    "        w_min_intra = 0.25, w_max_intra = 1.0,\n",
    "        w_min_inter = 0.25, w_max_inter = 1.0,\n",
    "        w_decay = 2.0,\n",
    "        sem_type = \"linear-gauss\", noise_scale  = 1,\n",
    "        max_data_gen_trials  = 3000\n",
    "        )\n",
    "    # w_mat = nx.to_numpy_array(g, nodelist=intra_nodes)\n",
    "    # a_mat = nx.to_numpy_array(g, nodelist=intra_nodes + inter_nodes)[len(intra_nodes) :, : len(intra_nodes)]\n",
    "    adj = nx.to_numpy_array(g, nodelist=intra_nodes + inter_nodes)\n",
    "    # X=XW+YA+Z 1*5=1*5 5*5 + 1*5p 5p* 5 + 1*5, adj = [w_mat 0; a_mat 0], adj[5:,:5]==a_mat\n",
    "    to_npy(file_path, i_dataset, np.where(adj==0,adj,1), df) # save CPDAG, DAG, data\n",
    "\n",
    "    target_list = _pick_targets(nb_nodes=len(intra_nodes), \n",
    "                                min_nb_target=min_nb_target, max_nb_target=max_nb_target, \n",
    "                                nb_interventions=nb_interventions)\n",
    "    print(\"target_list:\", target_list)\n",
    "\n",
    "    mask_intervention = []\n",
    "    regimes = []\n",
    "    mask_intervention.extend([[] for i in range(points_per_interv[0]*nb_time_points)])\n",
    "    regimes.extend([0 for i in range(points_per_interv[0]*nb_time_points)])\n",
    "    df_interv = copy.deepcopy(df) # pd.DataFrame(columns=df.columns)\n",
    "    # print(df_interv.shape)\n",
    "\n",
    "    # generate intervention dag & data_timesery\n",
    "    eta = 2 # 衰减系数\n",
    "    # cause = UniformCause(-0.2,0.2)\n",
    "    for j in range(nb_interventions):\n",
    "        targets = target_list[j]\n",
    "        mask_intervention.extend([targets for i in range(points_per_interv[j]*nb_time_points)])\n",
    "        regimes.extend([j+1 for i in range(points_per_interv[j]*nb_time_points)])\n",
    "        for k in range(points_per_interv[j]):\n",
    "            g_jk = copy.deepcopy(g)\n",
    "            g_jk = do_intervention(num_nodes, interv_type, intra_nodes, inter_nodes, eta, targets, g_jk)\n",
    "            df_jk = gen_df_from_g(g_jk, lag, intra_nodes, inter_nodes, num_nodes, n_samples=nb_time_points, \n",
    "                                max_data_gen_trials=10000, sem_type=\"linear-gauss\", noise_scale=1)\n",
    "            df_interv = pd.concat([df_interv, df_jk], axis=0)\n",
    "            print(j, k, df_interv.shape)\n",
    "    # print(len(regimes), len(mask_intervention))\n",
    "    _save_data(file_path, i_dataset-1, df_interv, regimes=regimes, mask=mask_intervention)  \n",
    "    return g, df, intra_nodes, inter_nodes, df_interv, regimes, mask_intervention\n",
    "\n",
    "def do_intervention(num_nodes, interv_type, intra_nodes, inter_nodes, eta, targets, g_jk):\n",
    "    for tn in targets:\n",
    "        if interv_type=='perfect':\n",
    "            for node in intra_nodes+inter_nodes:\n",
    "                g_jk.remove_edges_from([(node,f\"{tn}_lag0\")])\n",
    "        if interv_type=='imperfect':\n",
    "            for i in range(g_jk.number_of_nodes()):\n",
    "                p = i//num_nodes\n",
    "                org_edge = g_jk.get_edge_data(f\"{i%num_nodes}_lag{p}\",f\"{tn}_lag0\",default=0)\n",
    "                if org_edge!=0: \n",
    "                    change = np.random.uniform(0.25,0.5) # (2, 5)\n",
    "                    if np.random.randint(2) == 0: change *= -1\n",
    "                    if org_edge['weight']>0: \n",
    "                        new_w = org_edge['weight'] + ((1/eta)**p) * change # imperfect intervention,只在原有边基础上更改强度\n",
    "                    else: \n",
    "                        new_w = org_edge['weight'] - ((1/eta)**p) * change\n",
    "                    g_jk.add_weighted_edges_from([(f\"{i%num_nodes}_lag{p}\", f\"{tn}_lag0\",new_w)])\n",
    "    return g_jk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datasets = 1\n",
    "file_path = '/home/lipeiwen.lpw/temporal_intervention/data_timesery/imperfect/'\n",
    "nb_interventions = 2\n",
    "min_nb_target = 1\n",
    "max_nb_target = 1\n",
    "nb_all_sample = 3\n",
    "num_nodes = 58\n",
    "nb_time_points = 120\n",
    "lag = 1\n",
    "interv_type='imperfect'\n",
    "\n",
    "for i in range(0,num_datasets):\n",
    "    i_dataset = i+1\n",
    "    print('i_dataset:',i_dataset)\n",
    "    g, df, intra_nodes, inter_nodes, df_interv, regimes, mask_intervention = simulate_timedata(\n",
    "        i_dataset,\n",
    "        file_path = file_path + f'{num_nodes}_p{lag}_smp{nb_all_sample*nb_time_points}/',\n",
    "        nb_interventions = nb_interventions,\n",
    "        min_nb_target = min_nb_target, \n",
    "        max_nb_target = max_nb_target,\n",
    "        nb_all_sample = nb_all_sample,\n",
    "        num_nodes = num_nodes,\n",
    "        nb_time_points = nb_time_points,\n",
    "        lag = lag,\n",
    "        interv_type=interv_type\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g, df, intra_nodes, inter_nodes = gen_stationary_dyn_net_and_df(\n",
    "#         num_nodes = 58, n_samples = 120,\n",
    "#         p = 1, degree_intra = 3, degree_inter = 3,\n",
    "#         graph_type_intra = \"erdos-renyi\", graph_type_inter = \"erdos-renyi\",\n",
    "#         # w_min_intra = 0.5, w_max_intra = 2.0,\n",
    "#         # w_min_inter = 0.3, w_max_inter = 0.5,\n",
    "#         w_min_intra = 0.25, w_max_intra = 1.0,\n",
    "#         w_min_inter = 0.25, w_max_inter = 1.0,\n",
    "#         w_decay = 2.0,\n",
    "#         sem_type = \"linear-gauss\", noise_scale  = 1,\n",
    "#         max_data_gen_trials  = 3000\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
